{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRKyzXeBj3YrRFU1PA5/6X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badoil/ML/blob/master/multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8kfMtXIpg6jH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(axis=-1, keepdim=True)\n",
        "        std = x.std(axis=-1, keepdim=True)\n",
        "        return self.a_2 * (x-mean)/(std+eps) + self.b_2"
      ],
      "metadata": {
        "id": "4UQSWMuRg7ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "cliCmamehPJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module, N):\n",
        "    #module 반복할 클래스, N 반복횟수\n",
        "    return nn.ModuleList([deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "UiS99K7LhSGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attention\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    # Q, K, V : (batch_size, num_head, seq_length, d_k)  d_k 엠베딩 사이즈, 4차원 텐서\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # mask가 0인 애들(값이 0인 애들)을 아주작은 수로 바꿈\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "metadata": {
        "id": "mEubGcKXhY_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-head attention\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):   # h헤드의개수(8), d_model 모델 차원(512)\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clone(nn.Linear(d_model, d_model), 4)  # 뉴럴네트워크 4개\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Q, K, V : (batch_size, seq_length, d_model)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)  # h 헤드가 추가되므로 한 차원 늘림\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # Q, K, V tensor 만들기 and head 추가\n",
        "        # 아웃풋 (n_batch, h, seq_length, d_k)  -> Q, k, V\n",
        "        # 아래 코드는 각 헤드에 대한 Q,K,V 텐서\n",
        "        query, key, value = l(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2) for l, x in zip(self.linears, (query, key, value)):\n",
        "\n",
        "        # attention 구하기\n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
        "\n",
        "        # 헤드 갯수인 8개의 매트릭스를 하나로 일렬로 concat & linear transformation 통해서 최종 어텐션 스코어 계산\n",
        "        # x: (batch_size, num_head, seq_length, d_k)\n",
        "        # 최종적 tensor shape (batch_size, seq_length, num_head * d_k)\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h*self.d_k)  # concat\n",
        "        return self.linear[-1](x) # (batch_size, seq_length, d_model) d_model로 차원축소"
      ],
      "metadata": {
        "id": "q4NlgDsJhb2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## position-wise feed-forward network\n",
        "class PositionWiseFeedForwardNetworkclass(nn.Module):\n",
        "    def __init__(self, d_model, ff_model, dropout=0.1):\n",
        "        super(PositionWiseFeedForwardNetworkclass, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, ff_model)\n",
        "        self.w_2 = nn.Linear(ff_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "metadata": {
        "id": "KhU6uuajhfvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sublayer connection\n",
        "#1) layer norm\n",
        "#2) residual connection\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(x, sublayer):\n",
        "        return x + self.dropout(sublayer(x))  # sublayer: ex. mutiheaded attention(class), FFNN(class)"
      ],
      "metadata": {
        "id": "qNK1lXVQhiLg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}