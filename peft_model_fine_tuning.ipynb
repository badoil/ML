{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNt3iBtgqzxRi+T2XXeqcX8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badoil/ML/blob/master/peft_model_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xk2yCKlk242",
        "outputId": "e892ed07-400d-4fd9-bafb-a00f65c6fbd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: tokenizers, safetensors, xxhash, dill, responses, multiprocess, huggingface-hub, transformers, datasets, evaluate, accelerate, peft\n",
            "Successfully installed accelerate-0.22.0 datasets-2.14.5 dill-0.3.7 evaluate-0.4.0 huggingface-hub-0.16.4 multiprocess-0.70.15 peft-0.5.0 responses-0.18.0 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "TcQZrFs7la0q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "f9IP6ZvemBMd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Sxsg9M-BmLS5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJURC1gdmdUV",
        "outputId": "6231a6cf-ecf2-4c15-8b12-33833bbe55ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive-download-20230908T114816Z-001.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crXRtOK4mfFI",
        "outputId": "fb4c21a1-d5a3-4883-f126-8a1915eeab6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive-download-20230908T114816Z-001.zip\n",
            "  inflating: adapter_config.json     \n",
            "  inflating: tokenizer_config.json   \n",
            "  inflating: README.md               \n",
            "  inflating: special_tokens_map.json  \n",
            "  inflating: tokenizer.json          \n",
            "  inflating: adapter_model.bin       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset"
      ],
      "metadata": {
        "id": "tM82bO0Ro34R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)  # 원래 32float을 16float으로 경량화해서 모델 불러옴\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "k8G3r0sxqWRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameter_num = 0\n",
        "for name, param in original_model.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    parameter_num += param.numel()\n",
        "print(parameter_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btaib-8trv7d",
        "outputId": "dc33c2e3-67f9-49ac-df70-41b5bb3c9c78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "247577856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 5\n",
        "X = dataset['test'][i]['dialogue']\n",
        "Y = dataset['test'][i]['summary']\n",
        "prompt = f\"\"\"\n",
        "summarize the following dialogue\n",
        "{X}\n",
        "summary:\n",
        "\"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "WiuGdMCSszh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer(prompt, return_tensors = 'pt')\n",
        "input"
      ],
      "metadata": {
        "id": "41ExXhojt6iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = original_model.generate(\n",
        "    input['input_ids'],\n",
        "    max_new_tokens=200\n",
        ")[0]\n",
        "output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "output"
      ],
      "metadata": {
        "id": "tjhz8v9tufxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "Wzi_Pi09vwih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(dataset):\n",
        "  instruction = f\"\"\"summarize the following dialogue\"\"\"\n",
        "  end_prompt = f\"\"\"summary:\"\"\"\n",
        "\n",
        "  prompts = [f\"{instruction}\\n\\n{text}\\n\\n{end_prompt}\" for text in  dataset['dialogue']]\n",
        "  dataset['input_ids'] = tokenizer(prompts, padding=\"max_length\", truncation=True, return_tensors='pt').input_ids\n",
        "  dataset['labels'] = tokenizer(dataset['summary'], padding=\"max_length\", truncation=True, return_tensors='pt').input_ids\n",
        "  return dataset\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "ljKO60GrwN70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.remove_columns(['id', 'dialogue', 'summary', 'topic'])"
      ],
      "metadata": {
        "id": "JPTvXodhzyNx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "l9OeLlfZ1Q8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = './flant5_dialogsum_finetuned'\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = out_dir,\n",
        "    learning_rate = 1e-5,\n",
        "    num_train_epochs = 1,\n",
        "    weight_decay = 0.01,\n",
        "    logging_steps = 1,\n",
        "    max_steps = 1   # 1step 은 한번의 batch size 돌아갈때 의미, 파라미터가 2억개라 한번만 돌려야함\n",
        ")"
      ],
      "metadata": {
        "id": "8VqasuIg1SF9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = original_model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_dataset['train'],\n",
        "    eval_dataset = tokenized_dataset['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "mWIYIFZT3ZnI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Dq2bGVAE4Dec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_ft_t5 = trainer.model"
      ],
      "metadata": {
        "id": "2NMnTSqB4RHE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_ft_t5"
      ],
      "metadata": {
        "id": "MCGXZ7wV4rMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu memory 비우기\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "qdyGd1td4sXo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PEFT: Parameter Efficient Fine Tuning\n",
        "# LORA: LOw-Rank-Adaption-of-large language model\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel"
      ],
      "metadata": {
        "id": "i1Fs5_Bw6DE3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=['q', 'v'],\n",
        "    lora_dropout = 0.01,\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")"
      ],
      "metadata": {
        "id": "DDQlRMc78fMW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model, lora_config)"
      ],
      "metadata": {
        "id": "iIXXXvY09tsm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameter_num = 0\n",
        "for name, param in peft_model.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    parameter_num += param.numel()\n",
        "print(parameter_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJKueTUv9405",
        "outputId": "09b57245-633f-4a4c-9eb6-1758337039e6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3538944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = './peft_flant5_dialogsum_finetuned'\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = out_dir,\n",
        "    auto_find_batch_size = True,\n",
        "    learning_rate = 1e-3,  # 파라미터 많이 줄어서 le 좀 키워줌\n",
        "    num_train_epochs = 1,\n",
        "    weight_decay = 0.01,\n",
        "    # logging_steps = 1,\n",
        "    # max_steps = 1   # 1step 은 한번의 batch size 돌아갈때 의미, 파라미터가 2억개라 한번만 돌려야함\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = peft_model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_dataset['train'],\n",
        "    eval_dataset = tokenized_dataset['validation']\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Zl3DLRG1-bOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model from local\n",
        "peft_model_from_local = PeftModel.from_pretrained(AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16),\n",
        "                        '/content/peft_model', torch_dtype=torch.bfloat16, is_trainable=False)"
      ],
      "metadata": {
        "id": "1LioZUa8_9-r"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_from_local.to(device)"
      ],
      "metadata": {
        "id": "EQI4sVzVBTCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "9RfuudEuBtu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')"
      ],
      "metadata": {
        "id": "PZWSutU3B1jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sg0mledbCC7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}